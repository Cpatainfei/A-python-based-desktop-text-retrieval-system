{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Space Model Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read necessary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "with open(\"queries\",'r') as file:\n",
    "    query_dic = json.load(file)\n",
    "\n",
    "with open(\"documents\",\"r\") as file:\n",
    "    document_dic = json.load(file)\n",
    "    \n",
    "with open(\"Inverted_index\",\"r\") as file:\n",
    "    Inverted_index = json.load(file)\n",
    "\n",
    "with open(\"vocabulary\",\"r\") as file:\n",
    "    vocabulary = file.readline()\n",
    "    vocabulary = vocabulary.split(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Formulating the query vector with simple zero and one scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formulating the query vector with simple zero and one scheme\n",
    "\n",
    "def get_vector(tokens):\n",
    "    \n",
    "    Q = np.zeros(len(vocabulary))\n",
    "    \n",
    "    for token in tokens:\n",
    "        try:\n",
    "            ind = list(vocabulary).index(token)\n",
    "            Q[ind] = 1\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can formulate the query vector with tf_idf (need to change this in the main loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formulating the query vector with tf_idf \n",
    "\n",
    "def get_query_vector(tokens):\n",
    "    \n",
    "    all_words = []\n",
    "    \n",
    "    Q = np.zeros(len(vocabulary))\n",
    "    \n",
    "    for d in documents_dic.items():\n",
    "        for w in d[1]:\n",
    "            all_words.append(w)\n",
    "    \n",
    "    for token in tokens:\n",
    "        \n",
    "        try:\n",
    "            ind = list(vocabulary).index(token)\n",
    "            tf = all_words.count(token)/len(all_words)\n",
    "            df = len(Inverted_index[token])\n",
    "            idf = math.log(len(documents_dic.keys())/df)\n",
    "            tf_idf = tf * idf\n",
    "            \n",
    "            Q[ind] = tf_idf\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Find all the related Documents with given query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_documents(tokens):\n",
    "\n",
    "    Intersection = []\n",
    "    \n",
    "    for i in tokens:\n",
    "        \n",
    "        # If token was in the index\n",
    "        \n",
    "        if i in Inverted_index.keys():\n",
    "            \n",
    "            posting_list = Inverted_index[i]\n",
    "            Intersection.append(posting_list)\n",
    "                \n",
    "    \n",
    "    #intersection between relavent documents\n",
    "\n",
    "    lst = set(Intersection[0])\n",
    "\n",
    "    for i in range(len(Intersection)):\n",
    "\n",
    "        lst = set(Intersection[i])&lst\n",
    "\n",
    "    Intersection = list(lst)\n",
    "    \n",
    "    return Intersection\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. formulating the document vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vector(documents,tokens):\n",
    "    \n",
    "    # Since we just need to consider terms that occur in the query,\n",
    "    # insdead of computing all tf-idf vlaues in a document, we just compute them for query items\n",
    "    \n",
    "    # document dictionary for documents and their vectors\n",
    "    dic = {}\n",
    "    \n",
    "    for doc in documents:\n",
    "        d = document_dic[doc]\n",
    "        count = Counter(d)\n",
    "        query_vec = np.zeros(len(vocabulary))\n",
    "        \n",
    "        for token in tokens:\n",
    "            \n",
    "            if token in d:\n",
    "            \n",
    "                tf = count[token]/len(d)\n",
    "                df = len(Inverted_index[token])\n",
    "                idf = math.log(len(document_dic.keys())/df)\n",
    "                tf_idf = tf * idf\n",
    "\n",
    "                ind = list(vocabulary).index(token)\n",
    "                query_vec[ind] = tf_idf\n",
    "\n",
    "        dic[doc] = query_vec\n",
    "    \n",
    "    return dic\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Get document rank for a particular query based on previous functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(query):\n",
    "\n",
    "    result = {}\n",
    "    \n",
    "    query_vec = get_vector(query)\n",
    "    documents = find_documents(query)\n",
    "    document_vec = document_vector(documents,query)\n",
    "\n",
    "    for i in document_vec.items():\n",
    "        \n",
    "        # compute consine similary between a query and its related documents\n",
    "\n",
    "        score = np.dot(query_vec,i[1])/(np.linalg.norm(query_vec)*np.linalg.norm(i[1]))\n",
    "\n",
    "        result[i[0]] = score\n",
    "        \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank all the documents \n",
    "\n",
    "def rank_documents(scores):\n",
    "\n",
    "    #scores = cosine_sim(query)\n",
    "    rank = sorted(scores.items(), key=lambda x:x[1],reverse=True)\n",
    "    \n",
    "    return rank\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Output results file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"output/result_vsm.txt\",\"w\") as file:\n",
    "    for q in query_dic.items():\n",
    "        query_id = q[0]\n",
    "        query = q[1]\n",
    "        # Get rank scores\n",
    "        scores  = cosine_sim(query)\n",
    "        rank = rank_documents(scores)\n",
    "        c = 0\n",
    "        for i in rank:\n",
    "            # Write query id\n",
    "            file.write(query_id)\n",
    "            file.write(\" \")\n",
    "            # write iter\n",
    "            file.write(\"1\")\n",
    "            file.write(\" \")\n",
    "            # Write document id\n",
    "            file.write(i[0])\n",
    "            file.write(\" \")\n",
    "            # write rank (irrelevant information)\n",
    "            file.write(\"0\")\n",
    "            file.write(\" \")\n",
    "            # write similarity\n",
    "            file.write(str(i[1]))\n",
    "            file.write(\" \")\n",
    "            file.write(\"run1\")\n",
    "            file.write(\"\\n\")\n",
    "            # only show the top 100 results\n",
    "            c += 1\n",
    "            if c > 100:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
